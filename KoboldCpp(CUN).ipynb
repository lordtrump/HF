{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyO0w4P5XWICtDVsZCbRWyrK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordtrump/HF/blob/main/KoboldCpp(CUN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XwAIWlOtEpn8"
      },
      "outputs": [],
      "source": [
        "#@title <b> [1]  If you play on mobile, tap this to open music player and play the white noise to keep tab running in the background. or google will kill your api\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start block below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title <b>[2] Select model and etc</b> {\"display-mode\":\"form\"}\n",
        "\n",
        "import os\n",
        "import time\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU because you used it to often recently, this can take a few hours before they let you back in. Try again later or subscribe to Colab Pro for immediate access. (or change email if you are really desperate)⚠️\")\n",
        "import time\n",
        "from google.colab import runtime\n",
        "import json\n",
        "import re\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# @markdown ### DO NOT CLOSE THIS TAB WHILE USING, GOOGLE ANTI AFK WILL KILL API\n",
        "\n",
        "Model = \"M7-Evil (7B)\"  # @param [\"M7-Evil (7B)\",\"Daybreak-Kunoichi (7B)\",\"Daybreak-TheSpice (8B)\",\"Gemma2-Daybreak (9B)\",\"Magnum (12B)\",\"Sunfall-NemoMix (12B)\",\"Nemo-Sunfall (12B)\"]{allow-input: true}\n",
        "Context = \"4096\" # @param [\"65536\",\"32768\",\"24576\",\"16384\",\"12288\",\"8192\",\"6144\",\"4096\", \"3072\", \"2048\"]\n",
        "\n",
        "Instruct_Preset = \"alpaca\" # @param [\"None\",\"alpaca\", \"vicuna\", \"llama-3\", \"command-r\", \"chatml\", \"mistral\", \"gemma2\", \"metharme\"]\n",
        "# @markdown model will works best if using the same instruct it was trained for. certain model (e.g. all llama3 models) will become idiot if the instruct is mismatched. Please refer to the table above.\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Advanced setting\n",
        "Layers = 99 # @param {type:\"number\"}\n",
        "# @markdown Reduce layer will make generation slower, but it also save VRAM. So you can have more context size.\n",
        "KvCache = \"0\" # @param [\"0\", \"1\", \"2\"]\n",
        "# @markdown Kvcache will reduce a bit of respose quality to save VRAM. But it will reprocess prompt every time new message is sent.\n",
        "\n",
        "blayer = 99\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "force_redownload_kobold = True # @param {type:\"boolean\"}\n",
        "\n",
        "provider = \"Cloudflare\" # @param [\"Cloudflare\", \"Localtunnel\"]\n",
        "# @markdown ##### If stuck at \"Checking if the server is up...\" while using cloudflare, change provider to localtunnel may help\n",
        "# @markdown ##### Check force redownload if you try everything but it still not working.\n",
        "modellink = ''\n",
        "\n",
        "# declare context template\n",
        "\n",
        "premade_instruct = {\n",
        "    \"alpaca\": {\n",
        "        \"system_start\": \"\\n### Input: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\n### Instruction: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\n### Response: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"vicuna\": {\n",
        "        \"system_start\": \"\\nSYSTEM: \",\n",
        "        \"system_end\": \"\",\n",
        "        \"user_start\": \"\\nUSER: \",\n",
        "        \"user_end\": \"\",\n",
        "        \"assistant_start\": \"\\nASSISTANT: \",\n",
        "        \"assistant_end\": \"\",\n",
        "    },\n",
        "    \"llama-3\": {\n",
        "        \"system_start\": \"<|start_header_id|>system<|end_header_id|>\\n\\n\",\n",
        "        \"system_end\": \"<|eot_id|>\",\n",
        "        \"user_start\": \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "        \"user_end\": \"<|eot_id|>\",\n",
        "        \"assistant_start\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "        \"assistant_end\": \"<|eot_id|>\",\n",
        "    },\n",
        "    \"chatml\": {\n",
        "        \"system_start\": \"<|im_start|>system\",\n",
        "        \"system_end\": \"<|im_end|>\",\n",
        "        \"user_start\": \"<|im_start|>user\",\n",
        "        \"user_end\": \"<|im_end|>\",\n",
        "        \"assistant_start\": \"<|im_start|>assistant\",\n",
        "        \"assistant_end\": \"<|im_end|>\",\n",
        "    },\n",
        "    \"command-r\": {\n",
        "        \"system_start\": \"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\",\n",
        "        \"system_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"user_start\": \"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\",\n",
        "        \"user_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "        \"assistant_start\": \"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\",\n",
        "        \"assistant_end\": \"<|END_OF_TURN_TOKEN|>\",\n",
        "    },\n",
        "    \"mistral\":  {\n",
        "      \"system_start\": \"\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"[INST] \",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \" [/INST]\",\n",
        "      \"assistant_end\": \"</s> \"\n",
        "    },\n",
        "    \"gemma2\":{\n",
        "      \"system_start\": \"<start_of_turn>system\\n\",\n",
        "      \"system_end\": \"<end_of_turn>\\n\",\n",
        "      \"user_start\": \"<start_of_turn>user\\n\",\n",
        "      \"user_end\": \"<end_of_turn>\\n\",\n",
        "      \"assistant_start\": \"<start_of_turn>model\\n\",\n",
        "      \"assistant_end\": \"<end_of_turn>\\n\"\n",
        "    },\n",
        "    \"metharme\": {\n",
        "      \"system_start\": \"<|system|>\",\n",
        "      \"system_end\": \"\",\n",
        "      \"user_start\": \"<|user|>\",\n",
        "      \"user_end\": \"\",\n",
        "      \"assistant_start\": \"<|model>\",\n",
        "      \"assistant_end\": \"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "if \"https\" in Model:\n",
        "  modellink = Model\n",
        "else:\n",
        "  match Model:\n",
        "    case \"M7-Evil (7B)\" :\n",
        "        modellink = \"https://huggingface.co/InferenceIllusionist/M7-Evil-7b-GGUF/resolve/main/m7-evil-7b-Q6_K.gguf\"\n",
        "    case \"Daybreak-Kunoichi (7B)\" :\n",
        "        modellink = \"https://huggingface.co/crestf411/daybreak-kunoichi-2dpo-7b-gguf/resolve/main/daybreak-kunoichi-2dpo-7b-q8_0.gguf?download=true\"\n",
        "    case \"Daybreak-TheSpice (8B)\" :\n",
        "        modellink = \"https://huggingface.co/Vdr1/L3-daybreak-TheSpice-8b-v0.1.3-GGUF/resolve/main/L3-daybreak-TheSpice-8b-v0.1.3-Q4_K_M.gguf?download=true\"\n",
        "    case \"Gemma2-Daybreak (9B)\" :\n",
        "        modellink = \"https://huggingface.co/Vdr1/gemma2-9B-daybreak-v0.5-GGUF-Imatrix-IQ/resolve/main/gemma2-9B-daybreak-v0.5-Q4_K_S-imat.gguf?download=true\"\n",
        "    case \"Magnum (12B)\" :\n",
        "        modellink = \"https://huggingface.co/NikolayKozloff/magnum-12b-v2.5-kto-Q5_K_M-GGUF/resolve/main/magnum-12b-v2.5-kto-q5_k_m.gguf\"\n",
        "    case \"Sunfall-NemoMix (12B)\" :\n",
        "        modellink = \"https://huggingface.co/Vdr1/Sunfall-NemoMix-Unleashed-12B-v0.6.1-GGUF-IQ/resolve/main/Sunfall-NemoMix-Unleashed-12B-v0.6.1-Q4_K_M.gguf?download=true\"\n",
        "    case \"Nemo-Sunfall (12B)\" :\n",
        "        modellink = \"https://huggingface.co/Vdr1/nemo-sunfall-v0.6.1-GGUF-IQ/resolve/main/nemo-sunfall-v0.6.1-Q4_K_M.gguf?download=true\"\n",
        "\n",
        "\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n",
        "clear_output()\n",
        "\n",
        "modelname = modellink.split('/')[-1].split('.')[0]\n",
        "matched = re.search(r'/([^/]+)-GGUF/', modellink)\n",
        "remodelname = (\"koboldcpp/model\")\n",
        "if matched:\n",
        "    remodelname = f\"koboldcpp/{matched.group(1)}\"\n",
        "isModelExist = os.path.isfile('model.txt')\n",
        "oldmodel = \"\"\n",
        "if(isModelExist):\n",
        "  f = open(\"model.txt\", \"r\")\n",
        "  oldmodel = f.read()\n",
        "  print(f'oldmodel = {oldmodel}')\n",
        "\n",
        "if(oldmodel != modellink or isModelExist == False):\n",
        "  print('Download: '+ modellink)\n",
        "  !aria2c -c -x 16 -s 16 -k 1M -o model.gguf --download-result=default --allow-overwrite=true --file-allocation=none --console-log-level=error $modellink\n",
        "  # !aria2c -x 16 -s 16 -k 1M --allow-overwrite=\"true\" --summary-interval=5 $modellink -o model.gguf 2>&1 | grep -Ev 'Redirecting'\n",
        "  with open('model.txt', 'w') as f:\n",
        "      f.write(modellink)\n",
        "print(modelname)\n",
        "clear_output()\n",
        "\n",
        "#create instruct file\n",
        "with open(\"instruct.json\", \"w\") as f:\n",
        "    f.write(json.dumps(premade_instruct[Instruct_Preset], separators=(\",\", \":\")))\n",
        "\n",
        "\n",
        "if provider == \"Localtunnel\":\n",
        "  !npm install -g localtunnel\n",
        "  print('\\n')\n",
        "  !echo > nohup.out\n",
        "  !nohup lt --port 5001 &\n",
        "  print(\"Checking if the server is up...\\n\")\n",
        "  while True:\n",
        "      time.sleep(1)\n",
        "      with open('nohup.out', 'r') as f:\n",
        "        if 'your url is' in f.read():\n",
        "            print('=============================================================================')\n",
        "            print('please verify ip of colab in the loca.lt link before using it as kobold url')\n",
        "            print('colab ip: ', end='')\n",
        "            !curl ipecho.net/plain\n",
        "            !cat nohup.out\n",
        "            print('===========================================================================')\n",
        "            print(\"--------------------------\\nServer up!\")\n",
        "            break\n",
        "  print(\"--------------------------\\n\")\n",
        "\n",
        "tunneling = \"\"\n",
        "if provider == \"Cloudflare\":\n",
        "  tunneling = \"--remotetunnel\"\n",
        "!./koboldcpp_linux model.gguf --usecublas 0 mmq --multiuser --gpulayers $Layers --contextsize $Context --flashattention --hordemodelname $remodelname --quiet --remotetunnel --chatcompletionsadapter instruct.json"
      ],
      "metadata": {
        "id": "RHAIXb0GFVKr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}